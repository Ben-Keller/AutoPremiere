{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e27f65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import docx\n",
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz\n",
    "from collections import defaultdict\n",
    "from difflib import SequenceMatcher, get_close_matches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45412874",
   "metadata": {},
   "source": [
    "<h3> Transcripts to CSVs </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4d795f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_subtitle(text):\n",
    "    # Remove asterisks and text between brackets\n",
    "    text = re.sub(r'\\*|\\[.*?\\]', '', text)\n",
    "    text = re.sub(r'[\\u2012\\u2013\\u2014\\u2015]', '-', text)\n",
    "    text = text.replace(\";\",\":\")\n",
    "    return text.strip()\n",
    "\n",
    "def convert_language_to_csv(language, input_file, subtitles):\n",
    "    doc = docx.Document(input_file)\n",
    "    subtitle_block = \"\"\n",
    "    current_time_range = None\n",
    "    current_highlight_color = None\n",
    "    \n",
    "    speaker=speakerDict[input_file.split(\"/\")[-1].split(\" (\")[0].lower()]\n",
    "    \n",
    "    for paragraph in doc.paragraphs:\n",
    "        text = paragraph.text.strip()\n",
    "        text = clean_subtitle(text)  # Clean the subtitle text\n",
    "        # Check if the paragraph contains a time range\n",
    "        time_range_match = re.match(r'^(\\d+[:;]\\d+(?::\\d+){0,2}\\s*-\\s*\\d+[:;]\\d+(?::\\d+){0,2})\\s*\\**', text)\n",
    "        if time_range_match:\n",
    "            time_range = time_range_match.group(1)\n",
    "            # If there's a previous subtitle block, add it to the dictionary\n",
    "            if subtitle_block and current_time_range:\n",
    "                subtitles[current_time_range][language] = {\n",
    "                    \"text\": subtitle_block,\n",
    "                    \"highlight_color\": current_highlight_color,\n",
    "                    \"comments\": \"\",  # Initialize an empty \"Comments\" column\n",
    "                    \"speaker\":speaker\n",
    "                }\n",
    "            subtitle_block = \"\"\n",
    "            current_time_range = time_range\n",
    "            current_highlight_color = None  # Reset the highlight color for a new time range\n",
    "        else:\n",
    "            # Extract the highlight color for each run in the paragraph\n",
    "            for run in paragraph.runs:\n",
    "                if run.font.highlight_color is not None:\n",
    "                    current_highlight_color = run.font.highlight_color\n",
    "            subtitle_block += text + \"\\n\"\n",
    "\n",
    "    # Add the last subtitle block to the dictionary\n",
    "    if subtitle_block and current_time_range:\n",
    "        subtitles[current_time_range][language] = {\n",
    "            \"text\": subtitle_block,\n",
    "            \"highlight_color\": current_highlight_color,\n",
    "            \"comments\": \"\",  # Initialize an empty \"Comments\" column\n",
    "            \"speaker\":speaker\n",
    "        }\n",
    "\n",
    "def interview_to_csv(input_files, output_csv_file):\n",
    "    # Initialize a dictionary to store subtitles for all languages\n",
    "    all_subtitles = defaultdict(lambda: defaultdict(str))\n",
    "\n",
    "    # Iterate through each language and convert to CSV\n",
    "    for language, input_file in input_files.items():\n",
    "        convert_language_to_csv(language, input_file, all_subtitles)\n",
    "\n",
    "    # Write the combined subtitles to the CSV file\n",
    "    with open(output_csv_file, 'w', newline='', encoding='utf-8') as f:\n",
    "        csv_writer = csv.writer(f)\n",
    "        # Write the header row\n",
    "        header = [\"Start Time\", \"End Time\"] + list(input_files.keys()) + [\"Highlight\", \"Comments\",\"Speaker\"]\n",
    "        csv_writer.writerow(header)\n",
    "        # Write the data rows with all languages stacked\n",
    "        for time_range, language_subtitles in all_subtitles.items():\n",
    "            try:\n",
    "                start_time, end_time = map(str.strip, time_range.split(\"-\"))\n",
    "                highlight_color = language_subtitles[list(input_files.keys())[0]][\"highlight_color\"]\n",
    "                speaker=language_subtitles[list(input_files.keys())[0]][\"speaker\"]\n",
    "                row = [start_time, end_time] + [clean_subtitle(language_subtitles.get(lang, {\"text\": \"\"})[\"text\"]) for lang in input_files.keys()] + [highlight_color, \"\",speaker]\n",
    "                csv_writer.writerow(row)\n",
    "            except:\n",
    "                print(\"broken: \", time_range)#,language_subtitles)\n",
    "\n",
    "    print(f\"Conversion completed. Data saved to {output_csv_file}\")\n",
    "\n",
    "#this is temporary until the speaker names are included directly in the transcripts\n",
    "speakerDf = pd.read_csv('../02_Transcripts/Kogi/kogi-names.csv')\n",
    "speakerDict= speakerDf.set_index('Interview Code').to_dict()['Interviewees']\n",
    "\n",
    "    \n",
    "# # Test\n",
    "# input_files = {\n",
    "#     \"English\": \"../02_Transcripts/Wayuu/Transcripts (EN)/dunas (EN).docx\",\n",
    "#     \"Spanish\": \"../02_Transcripts/Wayuu/Transcripts (ES)/dunas (ES).docx\"\n",
    "# }\n",
    "# output_csv_file = \"../04_Interview CSV/dunas.csv\"\n",
    "\n",
    "# # Convert all languages to CSV using the pipeline function\n",
    "# interview_to_csv(input_files, output_csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d48d0dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coca\n",
      "Conversion completed. Data saved to ../04_Interview CSV/Kogi/coca.csv\n",
      "fuego\n",
      "Conversion completed. Data saved to ../04_Interview CSV/Kogi/fuego.csv\n",
      "tejiendo\n",
      "Conversion completed. Data saved to ../04_Interview CSV/Kogi/tejiendo.csv\n",
      "arregoces\n",
      "Conversion completed. Data saved to ../04_Interview CSV/Kogi/arregoces.csv\n"
     ]
    }
   ],
   "source": [
    "#run the processing pipeline overall interviews\n",
    "\n",
    "transcripts_dir = \"../02_Transcripts/Kogi/\"\n",
    "\n",
    "def get_languages_and_interviews(transcripts_dir):\n",
    "    interviews_dict = {}  # Dictionary to store interviews and their languages and file paths\n",
    "\n",
    "    for language_folder in os.listdir(transcripts_dir):\n",
    "        language_dir_path = os.path.join(transcripts_dir, language_folder)\n",
    "        \n",
    "        # Check if it's a valid language folder (e.g., \"Transcripts (EN)\")\n",
    "        if os.path.isdir(language_dir_path) and language_folder.startswith(\"Transcripts (\"):\n",
    "            language = language_folder.split(\" (\")[1].split(\")\")[0]\n",
    "            \n",
    "            for interview_file in os.listdir(language_dir_path):\n",
    "                if interview_file.endswith(\".docx\") and not os.path.basename(interview_file).startswith(\"~$\"):\n",
    "                    interview_name, ext = os.path.splitext(interview_file)\n",
    "                    interview_code = interview_name.split(\"(\")[0].strip().lower()  # Extract the interview code (e.g., \"dunas\")\n",
    "                    interview_path = os.path.join(language_dir_path, interview_file)\n",
    "                    \n",
    "                    # Create or update the entry for this interview code\n",
    "                    if interview_code not in interviews_dict:\n",
    "                        interviews_dict[interview_code] = {}\n",
    "                    \n",
    "                    # Append language and file path to the input_files dictionary\n",
    "                    interviews_dict[interview_code][language] = interview_path\n",
    "\n",
    "    return interviews_dict    \n",
    "\n",
    "def process_all_interviews(transcripts_dir, output_csv_dir):\n",
    "    # Get the combined dictionary of languages and interviews\n",
    "    interviews_dict = get_languages_and_interviews(transcripts_dir)\n",
    "    \n",
    "    for interview_code in interviews_dict.keys():\n",
    "        print(interview_code)\n",
    "        # Process the interview using the input_files and interview_code\n",
    "        output_csv_file = os.path.join(output_csv_dir, f\"{interview_code}.csv\")\n",
    "        interview_to_csv(interviews_dict[interview_code], output_csv_file)\n",
    "        \n",
    "\n",
    "# Output directory for CSV files\n",
    "output_csv_dir = \"../04_Interview CSV/Kogi/\"\n",
    "\n",
    "# Process all interviews in the transcripts directory\n",
    "process_all_interviews(transcripts_dir, output_csv_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64031e0b",
   "metadata": {},
   "source": [
    "<h3>CSVs to SRTs</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e642885f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'00:31:23,083'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def timecode_to_frames(text):\n",
    "    if len(text.split(\":\"))==2:\n",
    "        frames=(int(text.split(\":\")[0])*60+int(text.split(\":\")[1]))*24\n",
    "    elif len(text.split(\":\"))==3:\n",
    "        frames=(int(text.split(\":\")[0])*3600+int(text.split(\":\")[1])*60+int(text.split(\":\")[2]))*24\n",
    "    elif len(text.split(\":\"))==4:\n",
    "        frames=(int(text.split(\":\")[0])*3600+int(text.split(\":\")[1])*60+int(text.split(\":\")[2]))*24+int(text.split(\":\")[3])\n",
    "    else:\n",
    "        print(text+\"timecode parse error\")\n",
    "    return frames\n",
    "\n",
    "timecode_to_frames(\"00:31:23:02\")  #test\n",
    "\n",
    "\n",
    "def frames_to_srt_timecode(frames,fast=False):\n",
    "    if fast==True:\n",
    "        frames=frames*24/23.976 ##this is dumb, but solves the error when importing into premiere\n",
    "    frames=int(frames)\n",
    "    hours = frames // (3600*24)\n",
    "    remaining_frames = frames % (3600*24)\n",
    "    minutes=remaining_frames // (60*24)\n",
    "    remaining_frames=remaining_frames % (60*24)\n",
    "    seconds=remaining_frames // (24)\n",
    "    remaining_frames =remaining_frames %(24)\n",
    "    frames=remaining_frames\n",
    "\n",
    "    timecode=\"{:02d}\".format(hours)+\":\"+\"{:02d}\".format(minutes)+\":\"+\"{:02d}\".format(seconds)+\",\"+\"{:03d}\".format(int(frames/24*1000))\n",
    "\n",
    "    return timecode\n",
    "\n",
    "frames_to_srt_timecode(45194)   #test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a71ce7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fuego\n",
      "fuego is fast\n",
      "['ES', 'EN']\n",
      "coca\n",
      "coca is fast\n",
      "['ES', 'EN']\n",
      "tejiendo\n",
      "tejiendo is fast\n",
      "['ES', 'EN']\n",
      "arregoces\n",
      "arregoces is fast\n",
      "['ES', 'EN']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Input and output directories\n",
    "csv_dir = \"../04_Interview CSV/Kogi/\"\n",
    "subtitles_dir = \"../05_Subtitles/Kogi/\"\n",
    "\n",
    "#fastSubs=[\"romelia\",\"joaquin\",\"magalys electrico\",\"neko weaving\",\"weildler inside\",\"weildler outside\",\"bailarinas\"]\n",
    "\n",
    "fastSubs=[\"coca\",\"fuego\",\"arregoces\",\"tejiendo\"]\n",
    "\n",
    "# Function to convert a CSV file to SRT for multiple languages\n",
    "def csv_to_srt_multiple_languages(csv_file):\n",
    "\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    #this should be removed, only is needed because of an error in exporting videos at 23.976 vs 24fps\n",
    "    code=csv_file.split(\"/\")[-1].split(\".csv\")[0].lower()\n",
    "    print(code)\n",
    "    fast=False\n",
    "    if code in fastSubs:\n",
    "        fast=True\n",
    "        print(code,\"is fast\")\n",
    "\n",
    "    # Get the language columns dynamically (excluding specific columns)\n",
    "    exclude_columns = [\"Start Time\", \"End Time\", \"Highlight\", \"Comments\",\"Speaker\"]\n",
    "    language_columns = [col for col in df.columns if col not in exclude_columns]\n",
    "    language_columns.reverse()#this is to make english last,\n",
    "    \n",
    "    print(language_columns)\n",
    "\n",
    "    # Initialize a dictionary to store lines for each language\n",
    "    language_srt_lines = {col: [] for col in language_columns}\n",
    "    combined_srt_lines = []\n",
    "    counter = 1\n",
    "\n",
    "    # Loop through each row in the CSV\n",
    "    for index, row in df.iterrows():\n",
    "        # Format the timestamps in SRT format\n",
    "        srt_timecode = f\"{counter}\\n{frames_to_srt_timecode(timecode_to_frames(row['Start Time']),fast)} --> {frames_to_srt_timecode(timecode_to_frames(row['End Time']),fast)}\"\n",
    "\n",
    "        # Append the text for each language to their respective lines\n",
    "        for lang in language_columns:\n",
    "            language_srt_lines[lang].append(srt_timecode)\n",
    "            language_srt_lines[lang].append(f\"{row[lang]}\\n\")\n",
    "\n",
    "        # Append the text to the combined SRT\n",
    "        combined_srt_lines.append(srt_timecode)\n",
    "        combined_srt_lines.append('\\n----\\n'.join([str(row[lang]) for lang in language_columns]) + '\\n')\n",
    "\n",
    "        # Increment the counter\n",
    "        counter += 1\n",
    "\n",
    "    # Determine the output SRT file paths for combined and individual SRTs\n",
    "    base_filename = os.path.splitext(os.path.basename(csv_file))[0]\n",
    "    combined_langs = '-'.join(language_columns)\n",
    "    combined_srt_filename = f\"{base_filename} ({combined_langs}).srt\"\n",
    "    combined_srt_dir = os.path.join(subtitles_dir,\"SRT Export (\"+combined_langs+\")\")\n",
    "    os.makedirs(combined_srt_dir, exist_ok=True)\n",
    "\n",
    "    combined_srt_path= os.path.join(combined_srt_dir, combined_srt_filename)\n",
    "\n",
    "    # Create the folder for combined SRT\n",
    "    os.makedirs(subtitles_dir, exist_ok=True)\n",
    "\n",
    "    # Write the combined SRT file\n",
    "    with open(combined_srt_path, 'w', encoding='utf-8') as combined_srt_file:\n",
    "        combined_srt_file.write('\\n'.join(combined_srt_lines))\n",
    "\n",
    "    # Create folders for each language and write the individual SRT files\n",
    "    for lang in language_columns:\n",
    "        lang_output_dir = os.path.join(subtitles_dir, f\"SRT Export ({lang})\")\n",
    "        os.makedirs(lang_output_dir, exist_ok=True)\n",
    "        lang_srt_filename = f\"{base_filename} ({lang}).srt\"\n",
    "        lang_srt_path = os.path.join(lang_output_dir, lang_srt_filename)\n",
    "        with open(lang_srt_path, 'w', encoding='utf-8') as lang_srt_file:\n",
    "            lang_srt_file.write('\\n'.join(language_srt_lines[lang]))\n",
    "\n",
    "    #print(f'Combined SRT file \"{combined_srt_path}\" and individual SRT files have been created.')\n",
    "\n",
    "\n",
    "# Process all CSV files in the input directory\n",
    "for filename in os.listdir(csv_dir):\n",
    "    if filename.endswith('.csv'):\n",
    "        csv_file = os.path.join(csv_dir, filename)\n",
    "        csv_to_srt_multiple_languages(csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64b3435",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552e4cbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8518c1e1",
   "metadata": {},
   "source": [
    "<h3> Check status of txts, csv, xmls, srts </h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c885287e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           EN Transcripts  ES Transcripts  EN Subs  ES Subs   XMLs  \\\n",
      "code                                                                 \n",
      "fuego                True            True     True     True  False   \n",
      "tejiendo             True            True     True     True  False   \n",
      "coca                 True            True     True     True  False   \n",
      "arregoces            True            True     True     True  False   \n",
      "\n",
      "           Undefined Count  \n",
      "code                        \n",
      "fuego                   56  \n",
      "tejiendo                34  \n",
      "coca                    10  \n",
      "arregoces               35  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "def getCodesPresent(folderpath,ext):\n",
    "    codes = []\n",
    "    for filename in os.listdir(folderpath):\n",
    "        if filename.endswith(ext) and os.path.getsize(os.path.join(folderpath,filename))>1000:\n",
    "            code=filename.lower().split(ext.lower())[0].strip()\n",
    "            codes.append(code)\n",
    "    return codes\n",
    "\n",
    "enTexts=getCodesPresent('../02_Transcripts/Kogi/Transcripts (EN)/',' (EN).docx')\n",
    "esTexts=getCodesPresent('../02_Transcripts/Kogi/Transcripts (ES)/',' (ES).docx')\n",
    "xmls=getCodesPresent('../03_Interview XML/Kogi/','- final.xml')\n",
    "enSubs=getCodesPresent('../05_Subtitles/Kogi/SRT Export (EN)/',' (EN).srt')\n",
    "esSubs=getCodesPresent('../05_Subtitles/Kogi/SRT Export (ES)/',' (ES).srt')\n",
    "\n",
    "def getUndefinedCount(code):\n",
    "    try:\n",
    "        docx_file='../02_Transcripts/Kogi/Transcripts (EN)/'+code+' (EN).docx'  \n",
    "        search_string=\"xxx\"\n",
    "        count = 0\n",
    "        # Load the DOCX document\n",
    "        doc = docx.Document(docx_file)\n",
    "        # Iterate through paragraphs and search for the string (case-insensitive)\n",
    "        for paragraph in doc.paragraphs:\n",
    "            if search_string.lower() in paragraph.text.lower():\n",
    "                count += paragraph.text.lower().count(search_string.lower())\n",
    "        return count\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "lists_dict ={\"EN Transcripts\":enTexts,\n",
    "            \"ES Transcripts\":esTexts,\n",
    "            \"EN Subs\":enSubs,\n",
    "            \"ES Subs\":esSubs,\n",
    "            \"XMLs\":xmls}\n",
    "    \n",
    "\n",
    "# Create a DataFrame with unique codes\n",
    "unique_codes = list(set(code for sublist in lists_dict.values() for code in sublist))\n",
    "df = pd.DataFrame({'code': unique_codes})\n",
    "\n",
    "# Add columns for each list with True/False values\n",
    "for list_name, code_list in lists_dict.items():\n",
    "    df[list_name] = df['code'].isin(code_list)\n",
    "\n",
    "# Add column for count of XXX in document    \n",
    "df[\"Undefined Count\"]=df[\"code\"].apply(getUndefinedCount)\n",
    "    \n",
    "# Set 'code' as the index\n",
    "df.set_index('code', inplace=True)\n",
    " \n",
    "# Print the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50426b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b1229e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a701dfff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d7bb8fc",
   "metadata": {},
   "source": [
    "<h3> Convert Script to Csv </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "951d6f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Start Time     End Time                                           EN  \\\n",
      "0     00:00:06:11  00:00:08:00                                They come out   \n",
      "1     00:00:08:00  00:00:14:19                  mountains, drawing graphics   \n",
      "2     00:00:14:21  00:00:16:17                                          and   \n",
      "3     00:00:16:17  00:00:20:05                                   from xxxx.   \n",
      "4     00:00:20:07  00:00:22:06  And is it natural for it to look like that?   \n",
      "...           ...          ...                                          ...   \n",
      "2663  01:02:25:08  01:02:27:01                    And so it is, we are one.   \n",
      "2664  01:02:27:01  01:02:29:05      What differentiates us is the language,   \n",
      "2665  01:02:29:05  01:02:31:11                    the custom, the clothing.   \n",
      "2666  01:02:31:11  01:02:33:11               It's what makes the difference   \n",
      "2667  01:02:33:11  01:02:36:02                          but the bet is one.   \n",
      "\n",
      "                                       ES  Highlight  Comments    Speaker  \\\n",
      "0                                   Salen        NaN       NaN       jose   \n",
      "1           montañas, gráficas de dibujos        NaN       NaN       jose   \n",
      "2                                       y        NaN       NaN       jose   \n",
      "3                                de xxxx.        NaN       NaN       jose   \n",
      "4           ¿Y es natural que se vea así?        NaN       NaN       jose   \n",
      "...                                   ...        ...       ...        ...   \n",
      "2663                 Y así es, somos una.        NaN       NaN  arregoces   \n",
      "2664  Lo que nos diferencia es el idioma,        NaN       NaN  arregoces   \n",
      "2665          la costumbre, la vestidura.        NaN       NaN  arregoces   \n",
      "2666                Es lo que diferencia,        NaN       NaN  arregoces   \n",
      "2667              pero la apuesta es una.        NaN       NaN  arregoces   \n",
      "\n",
      "      Interview  \n",
      "0         fuego  \n",
      "1         fuego  \n",
      "2         fuego  \n",
      "3         fuego  \n",
      "4         fuego  \n",
      "...         ...  \n",
      "2663  arregoces  \n",
      "2664  arregoces  \n",
      "2665  arregoces  \n",
      "2666  arregoces  \n",
      "2667  arregoces  \n",
      "\n",
      "[2668 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "def concatenate_transcript_csvs(folder_path):\n",
    "\n",
    "    # Get a list of all CSV files in the folder\n",
    "    csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "    # Initialize an empty DataFrame\n",
    "    concatenated_df = pd.DataFrame()\n",
    "\n",
    "    # Iterate through CSV files and concatenate them\n",
    "    for csv_file in csv_files:\n",
    "        interview_code = os.path.splitext(csv_file)[0]  # Extract interview code from file name\n",
    "        csv_path = os.path.join(folder_path, csv_file)\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        # Add an \"Interview\" column with the interview code\n",
    "        df['Interview'] = interview_code\n",
    "\n",
    "        # Concatenate the dataframes\n",
    "        concatenated_df = pd.concat([concatenated_df, df], ignore_index=True)\n",
    "        \n",
    "    concatenated_df.to_csv(\"../04_Interview CSV/Kogi-all.csv\")\n",
    "\n",
    "    return concatenated_df\n",
    "\n",
    "# Example usage:\n",
    "folder_path = \"../04_Interview CSV/Kogi/\"\n",
    "concatenated_transcripts = concatenate_transcript_csvs(folder_path)\n",
    "print(concatenated_transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce5556e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function to convert different time formats to seconds (unchanged)\n",
    "def convert_time_to_seconds(time_str):\n",
    "    try:\n",
    "        if ':' in time_str:\n",
    "            parts = time_str.split(':')\n",
    "            if len(parts) == 2:\n",
    "                minutes, seconds = map(int, parts)\n",
    "                return minutes * 60 + seconds\n",
    "\n",
    "        parts = time_str.split(':')\n",
    "        if len(parts) == 3 or len(parts) == 4:\n",
    "            hours, minutes, seconds, *milliseconds = map(int, parts)\n",
    "            total_seconds = hours * 3600 + minutes * 60 + seconds\n",
    "            if milliseconds:\n",
    "                total_seconds += milliseconds[0] / 100\n",
    "            return total_seconds\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "    raise ValueError(f\"Unrecognized time format: {time_str}\")\n",
    "\n",
    "    # Function to convert time format to seconds\n",
    "def time_to_seconds(time_str):\n",
    "    time_obj = datetime.strptime(str(time_str), \"%M:%S\")\n",
    "    return time_obj.minute * 60 + time_obj.second\n",
    "    \n",
    "# Function to format time in MM:SS (unchanged)\n",
    "def format_time(seconds):\n",
    "    minutes, seconds = divmod(seconds, 60)\n",
    "    time= f\"{minutes:02d}:{seconds:02d}\"\n",
    "    return time\n",
    "\n",
    "def find_start_character_index(string, subsequence, cutoff=0.6):\n",
    "\n",
    "    # Initialize the start index.\n",
    "    start_index = -1\n",
    "\n",
    "    # Iterate through the string using a sliding window approach.\n",
    "    for i in range(len(string) - len(subsequence) + 1):\n",
    "        window = string[i:i + len(subsequence)]\n",
    "\n",
    "        # Use difflib's SequenceMatcher to calculate similarity.\n",
    "        similarity = SequenceMatcher(None, subsequence, window).ratio()\n",
    "\n",
    "        # If the similarity exceeds the cutoff, consider it a match.\n",
    "        if similarity >= cutoff:\n",
    "            start_index = i\n",
    "            break\n",
    "\n",
    "    return start_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9727d40d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def exportNewScriptRow(matched_row,text):\n",
    "    start_time_str = matched_row[\"Start Time\"]\n",
    "    start_time = convert_time_to_seconds(start_time_str)\n",
    "    transcript_duration=convert_time_to_seconds(matched_row[\"End Time\"]) - convert_time_to_seconds(matched_row[\"Start Time\"])                    \n",
    "    offset = find_start_character_index(matched_row[\"EN\"],text[:8], cutoff=.8)\n",
    "    seconds_per_letter =transcript_duration / len(matched_row[\"EN\"])\n",
    "    start_time+=offset*seconds_per_letter\n",
    "    duration_seconds = len(text) * seconds_per_letter\n",
    "    end_time = start_time + duration_seconds\n",
    "\n",
    "    start_time_mmss = format_time(int(start_time))\n",
    "    end_time_mmss = format_time(int(end_time))\n",
    "    return start_time_mmss,end_time_mmss,duration_seconds\n",
    "\n",
    "# Function to match script text with transcript\n",
    "def match_script_with_transcript(docx_file_path, transcript_df):\n",
    "    doc = docx.Document(docx_file_path)\n",
    "\n",
    "    speaker = \"\"\n",
    "    next_row={\"EN\":\"temp\"}\n",
    "    matched_row={\"EN\":\"temp\"}\n",
    "    transcript_lines = []\n",
    "    for line in doc.paragraphs:\n",
    "        text = str(line.text.strip())\n",
    "        if any(char.isalpha() for char in text):\n",
    "            match = re.match(r'\\*([^, ]+)', text)\n",
    "            if match:##if the line is defining a new speaker because it in eht form *Magalys\n",
    "                speaker = match.group(1)\n",
    "            else:\n",
    "\n",
    "                speaker_df=transcript_df[transcript_df[\"Speaker\"].str.contains(speaker.lower())]  \n",
    "\n",
    "                ##first try the next line after the previously searched for line\n",
    "                if text in next_row[\"EN\"]:\n",
    "                    matched_row=next_row\n",
    "                    \n",
    "                elif text in matched_row[\"EN\"]:\n",
    "                    matched_row=matched_row\n",
    "                    \n",
    "                else:\n",
    "                    closest_match = get_close_matches(text, speaker_df[\"EN\"].apply(float_to_str), n=1, cutoff=0.6)\n",
    "                    if closest_match:\n",
    "                        print(closest_match,text)\n",
    "\n",
    "                        matched_row = speaker_df[speaker_df[\"EN\"] == closest_match[0]].iloc[0]\n",
    "\n",
    "\n",
    "                if closest_match or next_row[\"EN\"]==text:\n",
    "\n",
    "                    #this is a hacky way to get the next row but i could figure out a good index method\n",
    "                    getNextRow=False\n",
    "                    for index,row in speaker_df.iterrows():\n",
    "                        if getNextRow:\n",
    "                            next_row=row\n",
    "                            getNextRow=False\n",
    "                        elif row[\"EN\"]==matched_row[\"EN\"]:\n",
    "                            getNextRow=True\n",
    "\n",
    "                    start_time_mmss,end_time_mmss,duration_seconds=exportNewScriptRow(matched_row,text)\n",
    "                    interview=matched_row[\"Interview\"]\n",
    "                else:\n",
    "                    start_time_mmss=\"\"\n",
    "                    end_time_mmss=\"\"\n",
    "                    duration_seconds=\"\"\n",
    "                    interview=\"can't find\"\n",
    "\n",
    "                transcript_lines.append([speaker,interview, start_time_mmss, end_time_mmss, duration_seconds, text])                \n",
    "\n",
    "    return transcript_lines\n",
    "\n",
    "# Function to write matched transcript to CSV (unchanged)\n",
    "def write_matched_transcript_to_csv(transcript_lines, output_csv_path):\n",
    "    with open(output_csv_path, \"w\", newline=\"\") as csvfile:\n",
    "        fieldnames = [\"Speaker\", \"Interview\",\"Start Time\", \"End Time\", \"Duration (Seconds)\", \"Transcript\"]\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for line in transcript_lines:\n",
    "            speaker, interview, start_time_mmss, end_time_mmss, duration_seconds, text = line\n",
    "            writer.writerow({\"Speaker\": speaker,\"Interview\":interview, \"Start Time\": start_time_mmss, \"End Time\": end_time_mmss, \"Duration (Seconds)\": duration_seconds, \"Transcript\": text})\n",
    "\n",
    "def float_to_str(value):\n",
    "    return str(value)\n",
    "            \n",
    "# #test\n",
    "# script_path = \"../01_Scripts/Wayuu/Dunas Script.docx\"\n",
    "# transcript_csv = pd.read_csv(\"../04_Interview CSV/Wayuu/dunas.csv\")\n",
    "# transcript_csv[\"Interview\"]=\"dunas\"\n",
    "# output_csv_path = \"../01_Scripts/Wayuu/dunas-script.csv\"\n",
    "\n",
    "# transcript_lines = match_script_with_transcript(script_path, transcript_csv)\n",
    "# print(transcript_lines)\n",
    "# write_matched_transcript_to_csv(transcript_lines, output_csv_path)\n",
    "# print(f\"CSV file with speaker, start time, end time, duration, and matched transcript created at {output_csv_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d12b70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d19231a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['These are,'] These are,\n",
      "['This is the control that regulates the load'] This is the control that regulates the load\n",
      "['Well Neko, you are entering the Macuira mountain range.\\nThese are land of the Arpushana (family lineage)'] Well Neko, you are entering the Macuira mountain range.\n",
      "[\"That's why I remain engraved in the stone of destiny\\nas they say, there, at sword point\"] That's why I remain engraved in the stone of destiny\n",
      "['Where now'] Where now***\n",
      "['They all go'] They all go***\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m concat_transcripts \u001b[38;5;241m=\u001b[39m concatenate_transcript_csvs(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../04_Interview CSV/Wayuu/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m output_csv_path \u001b[38;5;241m=\u001b[39m script_path[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m transcript_lines \u001b[38;5;241m=\u001b[39m \u001b[43mmatch_script_with_transcript\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscript_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_transcripts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m write_matched_transcript_to_csv(transcript_lines, output_csv_path)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36mmatch_script_with_transcript\u001b[0;34m(docx_file_path, transcript_df)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closest_match \u001b[38;5;129;01mor\u001b[39;00m next_row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEN\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m==\u001b[39mtext:\n\u001b[1;32m     49\u001b[0m \n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m#this is a hacky way to get the next row but i could figure out a good index method\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     getNextRow\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m index,row \u001b[38;5;129;01min\u001b[39;00m speaker_df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m getNextRow:\n\u001b[1;32m     54\u001b[0m             next_row\u001b[38;5;241m=\u001b[39mrow\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/frame.py:1324\u001b[0m, in \u001b[0;36mDataFrame.iterrows\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1322\u001b[0m klass \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_sliced\n\u001b[1;32m   1323\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues):\n\u001b[0;32m-> 1324\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[43mklass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m k, s\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/series.py:401\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    398\u001b[0m         data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39m_values\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    399\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 401\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndarray\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# GH#13296 we are dealing with a compound dtype, which\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;66;03m#  should be treated as 2D\u001b[39;00m\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    406\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot construct a Series from an ndarray with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    407\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompound dtype.  Use DataFrame instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    408\u001b[0m         )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# #test\n",
    "script_path = \"../01_Scripts/Wayuu/Magalys test.docx\"\n",
    "concat_transcripts = concatenate_transcript_csvs(\"../04_Interview CSV/Wayuu/\")\n",
    "output_csv_path = script_path[:-5]+\".csv\"\n",
    "transcript_lines = match_script_with_transcript(script_path, concat_transcripts)\n",
    "write_matched_transcript_to_csv(transcript_lines, output_csv_path)\n",
    "\n",
    "print(\"Done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12264372",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca7d59d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6284b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f27181",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
