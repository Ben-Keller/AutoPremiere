{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e27f65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import docx\n",
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz\n",
    "from collections import defaultdict\n",
    "from difflib import SequenceMatcher, get_close_matches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45412874",
   "metadata": {},
   "source": [
    "<h3> Transcripts to CSVs </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c4d795f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_subtitle(text):\n",
    "    # Remove asterisks and text between brackets\n",
    "    text = re.sub(r'\\*|\\[.*?\\]', '', text)\n",
    "    text = re.sub(r'[\\u2012\\u2013\\u2014\\u2015]', '-', text)\n",
    "    text = text.replace(\";\",\":\")\n",
    "    return text.strip()\n",
    "\n",
    "def convert_language_to_csv(language, input_file, subtitles):\n",
    "    doc = docx.Document(input_file)\n",
    "    subtitle_block = \"\"\n",
    "    current_time_range = None\n",
    "    current_highlight_color = None\n",
    "    \n",
    "    speaker=speakerDict[input_file.split(\"/\")[-1].split(\" (\")[0].lower()]\n",
    "    \n",
    "    for paragraph in doc.paragraphs:\n",
    "        text = paragraph.text.strip()\n",
    "        text = clean_subtitle(text)  # Clean the subtitle text\n",
    "        # Check if the paragraph contains a time range\n",
    "        time_range_match = re.match(r'^(\\d+[:;]\\d+(?::\\d+){0,2}\\s*-\\s*\\d+[:;]\\d+(?::\\d+){0,2})\\s*\\**', text)\n",
    "        if time_range_match:\n",
    "            time_range = time_range_match.group(1)\n",
    "            # If there's a previous subtitle block, add it to the dictionary\n",
    "            if subtitle_block and current_time_range:\n",
    "                subtitles[current_time_range][language] = {\n",
    "                    \"text\": subtitle_block,\n",
    "                    \"highlight_color\": current_highlight_color,\n",
    "                    \"comments\": \"\",  # Initialize an empty \"Comments\" column\n",
    "                    \"speaker\":speaker\n",
    "                }\n",
    "            subtitle_block = \"\"\n",
    "            current_time_range = time_range\n",
    "            current_highlight_color = None  # Reset the highlight color for a new time range\n",
    "        else:\n",
    "            # Extract the highlight color for each run in the paragraph\n",
    "            for run in paragraph.runs:\n",
    "                if run.font.highlight_color is not None:\n",
    "                    current_highlight_color = run.font.highlight_color\n",
    "            subtitle_block += text + \"\\n\"\n",
    "\n",
    "    # Add the last subtitle block to the dictionary\n",
    "    if subtitle_block and current_time_range:\n",
    "        subtitles[current_time_range][language] = {\n",
    "            \"text\": subtitle_block,\n",
    "            \"highlight_color\": current_highlight_color,\n",
    "            \"comments\": \"\",  # Initialize an empty \"Comments\" column\n",
    "            \"speaker\":speaker\n",
    "        }\n",
    "\n",
    "def interview_to_csv(input_files, output_csv_file):\n",
    "    # Initialize a dictionary to store subtitles for all languages\n",
    "    all_subtitles = defaultdict(lambda: defaultdict(str))\n",
    "\n",
    "    # Iterate through each language and convert to CSV\n",
    "    for language, input_file in input_files.items():\n",
    "        convert_language_to_csv(language, input_file, all_subtitles)\n",
    "\n",
    "    # Write the combined subtitles to the CSV file\n",
    "    with open(output_csv_file, 'w', newline='', encoding='utf-8') as f:\n",
    "        csv_writer = csv.writer(f)\n",
    "        # Write the header row\n",
    "        header = [\"Start Time\", \"End Time\"] + list(input_files.keys()) + [\"Highlight\", \"Comments\",\"Speaker\"]\n",
    "        csv_writer.writerow(header)\n",
    "        # Write the data rows with all languages stacked\n",
    "        for time_range, language_subtitles in all_subtitles.items():\n",
    "            try:\n",
    "                start_time, end_time = map(str.strip, time_range.split(\"-\"))\n",
    "                highlight_color = language_subtitles[list(input_files.keys())[0]][\"highlight_color\"]\n",
    "                speaker=language_subtitles[list(input_files.keys())[0]][\"speaker\"]\n",
    "                row = [start_time, end_time] + [clean_subtitle(language_subtitles.get(lang, {\"text\": \"\"})[\"text\"]) for lang in input_files.keys()] + [highlight_color, \"\",speaker]\n",
    "                csv_writer.writerow(row)\n",
    "            except:\n",
    "                print(\"broken: \", time_range)#,language_subtitles)\n",
    "\n",
    "    print(f\"Conversion completed. Data saved to {output_csv_file}\")\n",
    "\n",
    "#this is temporary until the speaker names are included directly in the transcripts\n",
    "speakerDf = pd.read_csv('../02_Transcripts/Wayuu/wayuu-names.csv')\n",
    "speakerDict= speakerDf.set_index('Interview Code').to_dict()['Interviewees']\n",
    "\n",
    "    \n",
    "# # Test\n",
    "# input_files = {\n",
    "#     \"English\": \"../02_Transcripts/Wayuu/Transcripts (EN)/dunas (EN).docx\",\n",
    "#     \"Spanish\": \"../02_Transcripts/Wayuu/Transcripts (ES)/dunas (ES).docx\"\n",
    "# }\n",
    "# output_csv_file = \"../04_Interview CSV/dunas.csv\"\n",
    "\n",
    "# # Convert all languages to CSV using the pipeline function\n",
    "# interview_to_csv(input_files, output_csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d48d0dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weildler inside\n",
      "Conversion completed. Data saved to ../04_Interview CSV/Wayuu/weildler inside.csv\n",
      "neko urbana\n",
      "Conversion completed. Data saved to ../04_Interview CSV/Wayuu/neko urbana.csv\n",
      "abuelo pescador\n",
      "Conversion completed. Data saved to ../04_Interview CSV/Wayuu/abuelo pescador.csv\n",
      "abuela pescadora\n",
      "Conversion completed. Data saved to ../04_Interview CSV/Wayuu/abuela pescadora.csv\n",
      "tejedora abuela\n",
      "Conversion completed. Data saved to ../04_Interview CSV/Wayuu/tejedora abuela.csv\n",
      "joaquin\n",
      "Conversion completed. Data saved to ../04_Interview CSV/Wayuu/joaquin.csv\n",
      "healing woman\n",
      "Conversion completed. Data saved to ../04_Interview CSV/Wayuu/healing woman.csv\n",
      "pinta abuela\n",
      "Conversion completed. Data saved to ../04_Interview CSV/Wayuu/pinta abuela.csv\n",
      "salinero oscar\n",
      "Conversion completed. Data saved to ../04_Interview CSV/Wayuu/salinero oscar.csv\n",
      "magalys hammock\n",
      "Conversion completed. Data saved to ../04_Interview CSV/Wayuu/magalys hammock.csv\n",
      "neko father\n",
      "Conversion completed. Data saved to ../04_Interview CSV/Wayuu/neko father.csv\n",
      "bailarinas\n",
      "Conversion completed. Data saved to ../04_Interview CSV/Wayuu/bailarinas.csv\n",
      "neko piedra\n",
      "Conversion completed. Data saved to ../04_Interview CSV/Wayuu/neko piedra.csv\n",
      "palabrero luis\n",
      "Conversion completed. Data saved to ../04_Interview CSV/Wayuu/palabrero luis.csv\n",
      "weildler outside\n",
      "Conversion completed. Data saved to ../04_Interview CSV/Wayuu/weildler outside.csv\n",
      "ana weaving\n",
      "Conversion completed. Data saved to ../04_Interview CSV/Wayuu/ana weaving.csv\n",
      "weildler pre\n",
      "Conversion completed. Data saved to ../04_Interview CSV/Wayuu/weildler pre.csv\n",
      "magalys electrico\n",
      "Conversion completed. Data saved to ../04_Interview CSV/Wayuu/magalys electrico.csv\n",
      "neko weaving\n",
      "Conversion completed. Data saved to ../04_Interview CSV/Wayuu/neko weaving.csv\n",
      "romelia\n",
      "Conversion completed. Data saved to ../04_Interview CSV/Wayuu/romelia.csv\n",
      "wolunka\n",
      "Conversion completed. Data saved to ../04_Interview CSV/Wayuu/wolunka.csv\n",
      "eliana\n",
      "Conversion completed. Data saved to ../04_Interview CSV/Wayuu/eliana.csv\n",
      "salinero young\n",
      "Conversion completed. Data saved to ../04_Interview CSV/Wayuu/salinero young.csv\n",
      "dunas\n",
      "Conversion completed. Data saved to ../04_Interview CSV/Wayuu/dunas.csv\n"
     ]
    }
   ],
   "source": [
    "#run the processing pipeline overall interviews\n",
    "\n",
    "transcripts_dir = \"../02_Transcripts/Wayuu/\"\n",
    "\n",
    "def get_languages_and_interviews(transcripts_dir):\n",
    "    interviews_dict = {}  # Dictionary to store interviews and their languages and file paths\n",
    "\n",
    "    for language_folder in os.listdir(transcripts_dir):\n",
    "        language_dir_path = os.path.join(transcripts_dir, language_folder)\n",
    "        \n",
    "        # Check if it's a valid language folder (e.g., \"Transcripts (EN)\")\n",
    "        if os.path.isdir(language_dir_path) and language_folder.startswith(\"Transcripts (\"):\n",
    "            language = language_folder.split(\" (\")[1].split(\")\")[0]\n",
    "            \n",
    "            for interview_file in os.listdir(language_dir_path):\n",
    "                if interview_file.endswith(\".docx\") and not os.path.basename(interview_file).startswith(\"~$\"):\n",
    "                    interview_name, ext = os.path.splitext(interview_file)\n",
    "                    interview_code = interview_name.split(\"(\")[0].strip().lower()  # Extract the interview code (e.g., \"dunas\")\n",
    "                    interview_path = os.path.join(language_dir_path, interview_file)\n",
    "                    \n",
    "                    # Create or update the entry for this interview code\n",
    "                    if interview_code not in interviews_dict:\n",
    "                        interviews_dict[interview_code] = {}\n",
    "                    \n",
    "                    # Append language and file path to the input_files dictionary\n",
    "                    interviews_dict[interview_code][language] = interview_path\n",
    "\n",
    "    return interviews_dict    \n",
    "\n",
    "def process_all_interviews(transcripts_dir, output_csv_dir):\n",
    "    # Get the combined dictionary of languages and interviews\n",
    "    interviews_dict = get_languages_and_interviews(transcripts_dir)\n",
    "    \n",
    "    for interview_code in interviews_dict.keys():\n",
    "        print(interview_code)\n",
    "        # Process the interview using the input_files and interview_code\n",
    "        output_csv_file = os.path.join(output_csv_dir, f\"{interview_code}.csv\")\n",
    "        interview_to_csv(interviews_dict[interview_code], output_csv_file)\n",
    "        \n",
    "\n",
    "# Output directory for CSV files\n",
    "output_csv_dir = \"../04_Interview CSV/Wayuu/\"\n",
    "\n",
    "# Process all interviews in the transcripts directory\n",
    "process_all_interviews(transcripts_dir, output_csv_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64031e0b",
   "metadata": {},
   "source": [
    "<h3>CSVs to SRTs</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e642885f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'00:31:23,083'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def timecode_to_frames(text):\n",
    "    if len(text.split(\":\"))==2:\n",
    "        frames=(int(text.split(\":\")[0])*60+int(text.split(\":\")[1]))*24\n",
    "    elif len(text.split(\":\"))==3:\n",
    "        frames=(int(text.split(\":\")[0])*3600+int(text.split(\":\")[1])*60+int(text.split(\":\")[2]))*24\n",
    "    elif len(text.split(\":\"))==4:\n",
    "        frames=(int(text.split(\":\")[0])*3600+int(text.split(\":\")[1])*60+int(text.split(\":\")[2]))*24+int(text.split(\":\")[3])\n",
    "    else:\n",
    "        print(text+\"timecode parse error\")\n",
    "    return frames\n",
    "\n",
    "timecode_to_frames(\"00:31:23:02\")  #test\n",
    "\n",
    "\n",
    "def frames_to_srt_timecode(frames,fast=False):\n",
    "    if fast==True:\n",
    "        frames=frames*24/23.976 ##this is dumb, but solves the error when importing into premiere\n",
    "    frames=int(frames)\n",
    "    hours = frames // (3600*24)\n",
    "    remaining_frames = frames % (3600*24)\n",
    "    minutes=remaining_frames // (60*24)\n",
    "    remaining_frames=remaining_frames % (60*24)\n",
    "    seconds=remaining_frames // (24)\n",
    "    remaining_frames =remaining_frames %(24)\n",
    "    frames=remaining_frames\n",
    "\n",
    "    timecode=\"{:02d}\".format(hours)+\":\"+\"{:02d}\".format(minutes)+\":\"+\"{:02d}\".format(seconds)+\",\"+\"{:03d}\".format(int(frames/24*1000))\n",
    "\n",
    "    return timecode\n",
    "\n",
    "frames_to_srt_timecode(45194)   #test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a71ce7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salinero oscar\n",
      "['Speaker', 'ES', 'EN']\n",
      "dunas\n",
      "['Speaker', 'ES', 'EN']\n",
      "neko piedra\n",
      "['Speaker', 'ES', 'EN']\n",
      "tejedora abuela\n",
      "['Speaker', 'ES', 'EN']\n",
      "ana weaving\n",
      "['Speaker', 'ES', 'EN']\n",
      "healing woman\n",
      "['Speaker', 'ES', 'EN']\n",
      "wolunka\n",
      "['Speaker', 'ES', 'EN']\n",
      "magalys electrico\n",
      "magalys electrico is fast\n",
      "['Speaker', 'ES', 'EN']\n",
      "abuela pescadora\n",
      "['Speaker', 'ES', 'EN']\n",
      "romelia\n",
      "romelia is fast\n",
      "['Speaker', 'ES', 'EN']\n",
      "neko weaving\n",
      "neko weaving is fast\n",
      "['Speaker', 'ES', 'EN']\n",
      "neko father\n",
      "['Speaker', 'ES', 'EN']\n",
      "joaquin\n",
      "joaquin is fast\n",
      "['Speaker', 'ES', 'EN']\n",
      "neko urbana\n",
      "['Speaker', 'ES', 'EN']\n",
      "salinero young\n",
      "['Speaker', 'ES', 'EN']\n",
      "eliana\n",
      "['Speaker', 'ES', 'EN']\n",
      "bailarinas\n",
      "bailarinas is fast\n",
      "['Speaker', 'ES', 'EN']\n",
      "weildler pre\n",
      "['Speaker', 'ES', 'EN']\n",
      "abuelo pescador\n",
      "['Speaker', 'ES', 'EN']\n",
      "pinta abuela\n",
      "['Speaker', 'ES', 'EN']\n",
      "weildler outside\n",
      "weildler outside is fast\n",
      "['Speaker', 'ES', 'EN']\n",
      "palabrero luis\n",
      "['Speaker', 'ES', 'EN']\n",
      "weildler inside\n",
      "weildler inside is fast\n",
      "['Speaker', 'ES', 'EN']\n",
      "magalys hammock\n",
      "['Speaker', 'ES', 'EN']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Input and output directories\n",
    "csv_dir = \"../04_Interview CSV/Wayuu/\"\n",
    "subtitles_dir = \"../05_Subtitles/Wayuu/\"\n",
    "\n",
    "fastSubs=[\"romelia\",\"joaquin\",\"magalys electrico\",\"neko weaving\",\"weildler inside\",\"weildler outside\",\"bailarinas\"]\n",
    "\n",
    "# Function to convert a CSV file to SRT for multiple languages\n",
    "def csv_to_srt_multiple_languages(csv_file):\n",
    "\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    #this should be removed, only is needed because of an error in exporting videos at 23.976 vs 24fps\n",
    "    code=csv_file.split(\"/\")[-1].split(\".csv\")[0].lower()\n",
    "    print(code)\n",
    "    fast=False\n",
    "    if code in fastSubs:\n",
    "        fast=True\n",
    "        print(code,\"is fast\")\n",
    "\n",
    "    # Get the language columns dynamically (excluding specific columns)\n",
    "    exclude_columns = [\"Start Time\", \"End Time\", \"Highlight\", \"Comments\"]\n",
    "    language_columns = [col for col in df.columns if col not in exclude_columns]\n",
    "    language_columns.reverse()#this is to make english last,\n",
    "    \n",
    "    print(language_columns)\n",
    "\n",
    "    # Initialize a dictionary to store lines for each language\n",
    "    language_srt_lines = {col: [] for col in language_columns}\n",
    "    combined_srt_lines = []\n",
    "    counter = 1\n",
    "\n",
    "    # Loop through each row in the CSV\n",
    "    for index, row in df.iterrows():\n",
    "        # Format the timestamps in SRT format\n",
    "        srt_timecode = f\"{counter}\\n{frames_to_srt_timecode(timecode_to_frames(row['Start Time']),fast)} --> {frames_to_srt_timecode(timecode_to_frames(row['End Time']),fast)}\"\n",
    "\n",
    "        # Append the text for each language to their respective lines\n",
    "        for lang in language_columns:\n",
    "            language_srt_lines[lang].append(srt_timecode)\n",
    "            language_srt_lines[lang].append(f\"{row[lang]}\\n\")\n",
    "\n",
    "        # Append the text to the combined SRT\n",
    "        combined_srt_lines.append(srt_timecode)\n",
    "        combined_srt_lines.append('\\n----\\n'.join([str(row[lang]) for lang in language_columns]) + '\\n')\n",
    "\n",
    "        # Increment the counter\n",
    "        counter += 1\n",
    "\n",
    "    # Determine the output SRT file paths for combined and individual SRTs\n",
    "    base_filename = os.path.splitext(os.path.basename(csv_file))[0]\n",
    "    combined_langs = '-'.join(language_columns)\n",
    "    combined_srt_filename = f\"{base_filename} ({combined_langs}).srt\"\n",
    "    combined_srt_dir = os.path.join(subtitles_dir,\"SRT Export (\"+combined_langs+\")\")\n",
    "    os.makedirs(combined_srt_dir, exist_ok=True)\n",
    "\n",
    "    combined_srt_path= os.path.join(combined_srt_dir, combined_srt_filename)\n",
    "\n",
    "    # Create the folder for combined SRT\n",
    "    os.makedirs(subtitles_dir, exist_ok=True)\n",
    "\n",
    "    # Write the combined SRT file\n",
    "    with open(combined_srt_path, 'w', encoding='utf-8') as combined_srt_file:\n",
    "        combined_srt_file.write('\\n'.join(combined_srt_lines))\n",
    "\n",
    "    # Create folders for each language and write the individual SRT files\n",
    "    for lang in language_columns:\n",
    "        lang_output_dir = os.path.join(subtitles_dir, f\"SRT Export ({lang})\")\n",
    "        os.makedirs(lang_output_dir, exist_ok=True)\n",
    "        lang_srt_filename = f\"{base_filename} ({lang}).srt\"\n",
    "        lang_srt_path = os.path.join(lang_output_dir, lang_srt_filename)\n",
    "        with open(lang_srt_path, 'w', encoding='utf-8') as lang_srt_file:\n",
    "            lang_srt_file.write('\\n'.join(language_srt_lines[lang]))\n",
    "\n",
    "    #print(f'Combined SRT file \"{combined_srt_path}\" and individual SRT files have been created.')\n",
    "\n",
    "\n",
    "# Process all CSV files in the input directory\n",
    "for filename in os.listdir(csv_dir):\n",
    "    if filename.endswith('.csv'):\n",
    "        csv_file = os.path.join(csv_dir, filename)\n",
    "        csv_to_srt_multiple_languages(csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64b3435",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552e4cbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8518c1e1",
   "metadata": {},
   "source": [
    "<h3> Check status of txts, csv, xmls, srts </h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c885287e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   EN Transcripts  ES Transcripts  EN Subs  ES Subs   XMLs  \\\n",
      "code                                                                         \n",
      "neko urbana                  True            True     True     True   True   \n",
      "palabrero luis               True            True     True     True   True   \n",
      "magalys electrico            True            True     True     True   True   \n",
      "neko weaving                 True            True     True     True   True   \n",
      "salinero oscar               True            True     True     True  False   \n",
      "romelia                      True            True     True     True   True   \n",
      "weildler outside             True            True     True     True   True   \n",
      "tejedora abuela              True            True     True     True   True   \n",
      "salinero young               True            True     True     True   True   \n",
      "neko piedra                  True            True     True     True   True   \n",
      "weildler pre                 True            True     True     True  False   \n",
      "healing woman                True            True     True     True   True   \n",
      "pinta abuela                 True            True     True     True   True   \n",
      "dunas                        True            True     True     True  False   \n",
      "quatro mujeres              False           False    False    False   True   \n",
      "abuelo pescador              True            True     True     True   True   \n",
      "neko father                  True            True     True     True   True   \n",
      "ana weaving                  True            True     True     True   True   \n",
      "wolunka                      True            True     True     True   True   \n",
      "weildler inside              True            True     True     True   True   \n",
      "bailarinas                   True            True     True     True   True   \n",
      "magalys hammock              True            True     True     True   True   \n",
      "abuela pescadora             True            True     True     True   True   \n",
      "joaquin                      True            True     True     True   True   \n",
      "eliana                       True            True     True     True   True   \n",
      "\n",
      "                  Undefined Count  \n",
      "code                               \n",
      "neko urbana                    15  \n",
      "palabrero luis                  0  \n",
      "magalys electrico               0  \n",
      "neko weaving                    0  \n",
      "salinero oscar                  0  \n",
      "romelia                         0  \n",
      "weildler outside                0  \n",
      "tejedora abuela                 0  \n",
      "salinero young                  0  \n",
      "neko piedra                     0  \n",
      "weildler pre                    5  \n",
      "healing woman                   0  \n",
      "pinta abuela                    0  \n",
      "dunas                           0  \n",
      "quatro mujeres                     \n",
      "abuelo pescador                 0  \n",
      "neko father                     0  \n",
      "ana weaving                    35  \n",
      "wolunka                         0  \n",
      "weildler inside                 0  \n",
      "bailarinas                      0  \n",
      "magalys hammock                 0  \n",
      "abuela pescadora                0  \n",
      "joaquin                         0  \n",
      "eliana                          0  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "def getCodesPresent(folderpath,ext):\n",
    "    codes = []\n",
    "    for filename in os.listdir(folderpath):\n",
    "        if filename.endswith(ext) and os.path.getsize(os.path.join(folderpath,filename))>1000:\n",
    "            code=filename.lower().split(ext.lower())[0].strip()\n",
    "            codes.append(code)\n",
    "    return codes\n",
    "\n",
    "enTexts=getCodesPresent('../02_Transcripts/Wayuu/Transcripts (EN)/',' (EN).docx')\n",
    "esTexts=getCodesPresent('../02_Transcripts/Wayuu/Transcripts (ES)/',' (ES).docx')\n",
    "xmls=getCodesPresent('../03_Interview XML/Wayuu/','- final.xml')\n",
    "enSubs=getCodesPresent('../05_Subtitles/Wayuu/SRT Export (EN)/',' (EN).srt')\n",
    "esSubs=getCodesPresent('../05_Subtitles/Wayuu/SRT Export (ES)/',' (ES).srt')\n",
    "\n",
    "def getUndefinedCount(code):\n",
    "    try:\n",
    "        docx_file='../02_Transcripts/Wayuu/Transcripts (EN)/'+code+' (EN).docx'  \n",
    "        search_string=\"xxx\"\n",
    "        count = 0\n",
    "        # Load the DOCX document\n",
    "        doc = docx.Document(docx_file)\n",
    "        # Iterate through paragraphs and search for the string (case-insensitive)\n",
    "        for paragraph in doc.paragraphs:\n",
    "            if search_string.lower() in paragraph.text.lower():\n",
    "                count += paragraph.text.lower().count(search_string.lower())\n",
    "        return count\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "lists_dict ={\"EN Transcripts\":enTexts,\n",
    "            \"ES Transcripts\":esTexts,\n",
    "            \"EN Subs\":enSubs,\n",
    "            \"ES Subs\":esSubs,\n",
    "            \"XMLs\":xmls}\n",
    "    \n",
    "\n",
    "# Create a DataFrame with unique codes\n",
    "unique_codes = list(set(code for sublist in lists_dict.values() for code in sublist))\n",
    "df = pd.DataFrame({'code': unique_codes})\n",
    "\n",
    "# Add columns for each list with True/False values\n",
    "for list_name, code_list in lists_dict.items():\n",
    "    df[list_name] = df['code'].isin(code_list)\n",
    "\n",
    "# Add column for count of XXX in document    \n",
    "df[\"Undefined Count\"]=df[\"code\"].apply(getUndefinedCount)\n",
    "    \n",
    "# Set 'code' as the index\n",
    "df.set_index('code', inplace=True)\n",
    " \n",
    "# Print the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50426b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b1229e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a701dfff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d7bb8fc",
   "metadata": {},
   "source": [
    "<h3> Convert Script to Csv </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "951d6f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Start Time End Time                                                 EN  \\\n",
      "0          00:04    00:06        Tell us your name what do you do for living   \n",
      "1          00:07    00:10                         My name is Oscar Pushaina.   \n",
      "2          00:11    00:12                           And what do you do here?   \n",
      "3          00:13    00:16                      I come here to work with salt   \n",
      "4          00:17    00:22  Sometimes I arrive here at dawn, at 4 in the m...   \n",
      "...          ...      ...                                                ...   \n",
      "10310      49:31    49:34     And when people come here, I like to help them   \n",
      "10311      49:36    49:39                         and the book? Oh the book.   \n",
      "10312      49:40    49:45  Well we wrote a book with Juan David with the ...   \n",
      "10313      49:48    49:51               About energy and what I did in India   \n",
      "10314      49:57    50:02           Anayawasü (thank you) Jatsü (it's over).   \n",
      "\n",
      "                                                      ES Highlight  Comments  \\\n",
      "0                      Dinos su nombre y a que se dedica       NaN       NaN   \n",
      "1                                Me llamo Oscar Pushaina       NaN       NaN   \n",
      "2                                ¿Y a que se dedica acá?       NaN       NaN   \n",
      "3                     Yo vengo acá a trabajar con la sal       NaN       NaN   \n",
      "4      A veces yo llego aquí en la madrugada, desde l...       NaN       NaN   \n",
      "...                                                  ...       ...       ...   \n",
      "10310  Y las personas cuando llegan aquí, me gusta ay...       NaN       NaN   \n",
      "10311                         ¿Y el libro? Ah, el libro.       NaN       NaN   \n",
      "10312  Bueno escribimos un libro con Juan David con l...       NaN       NaN   \n",
      "10313         Sobre la energía y lo que hice en la India       NaN       NaN   \n",
      "10314              Anayawasü (gracias) Jatsü (se acabó).       NaN       NaN   \n",
      "\n",
      "              Speaker        Interview  \n",
      "0      salinero oscar   salinero oscar  \n",
      "1      salinero oscar   salinero oscar  \n",
      "2      salinero oscar   salinero oscar  \n",
      "3      salinero oscar   salinero oscar  \n",
      "4      salinero oscar   salinero oscar  \n",
      "...               ...              ...  \n",
      "10310         magalys  Magalys Hammock  \n",
      "10311         magalys  Magalys Hammock  \n",
      "10312         magalys  Magalys Hammock  \n",
      "10313         magalys  Magalys Hammock  \n",
      "10314         magalys  Magalys Hammock  \n",
      "\n",
      "[10315 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "def concatenate_transcript_csvs(folder_path):\n",
    "\n",
    "    # Get a list of all CSV files in the folder\n",
    "    csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "    # Initialize an empty DataFrame\n",
    "    concatenated_df = pd.DataFrame()\n",
    "\n",
    "    # Iterate through CSV files and concatenate them\n",
    "    for csv_file in csv_files:\n",
    "        interview_code = os.path.splitext(csv_file)[0]  # Extract interview code from file name\n",
    "        csv_path = os.path.join(folder_path, csv_file)\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        # Add an \"Interview\" column with the interview code\n",
    "        df['Interview'] = interview_code\n",
    "\n",
    "        # Concatenate the dataframes\n",
    "        concatenated_df = pd.concat([concatenated_df, df], ignore_index=True)\n",
    "        \n",
    "    concatenated_df.to_csv(\"../04_Interview CSV/Wayuu-all.csv\")\n",
    "\n",
    "    return concatenated_df\n",
    "\n",
    "# Example usage:\n",
    "folder_path = \"../04_Interview CSV/Wayuu/\"\n",
    "concatenated_transcripts = concatenate_transcript_csvs(folder_path)\n",
    "print(concatenated_transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "ce5556e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function to convert different time formats to seconds (unchanged)\n",
    "def convert_time_to_seconds(time_str):\n",
    "    try:\n",
    "        if ':' in time_str:\n",
    "            parts = time_str.split(':')\n",
    "            if len(parts) == 2:\n",
    "                minutes, seconds = map(int, parts)\n",
    "                return minutes * 60 + seconds\n",
    "\n",
    "        parts = time_str.split(':')\n",
    "        if len(parts) == 3 or len(parts) == 4:\n",
    "            hours, minutes, seconds, *milliseconds = map(int, parts)\n",
    "            total_seconds = hours * 3600 + minutes * 60 + seconds\n",
    "            if milliseconds:\n",
    "                total_seconds += milliseconds[0] / 100\n",
    "            return total_seconds\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "    raise ValueError(f\"Unrecognized time format: {time_str}\")\n",
    "\n",
    "    # Function to convert time format to seconds\n",
    "def time_to_seconds(time_str):\n",
    "    time_obj = datetime.strptime(str(time_str), \"%M:%S\")\n",
    "    return time_obj.minute * 60 + time_obj.second\n",
    "    \n",
    "# Function to format time in MM:SS (unchanged)\n",
    "def format_time(seconds):\n",
    "    minutes, seconds = divmod(seconds, 60)\n",
    "    time= f\"{minutes:02d}:{seconds:02d}\"\n",
    "    return time\n",
    "\n",
    "def find_start_character_index(string, subsequence, cutoff=0.6):\n",
    "\n",
    "    # Initialize the start index.\n",
    "    start_index = -1\n",
    "\n",
    "    # Iterate through the string using a sliding window approach.\n",
    "    for i in range(len(string) - len(subsequence) + 1):\n",
    "        window = string[i:i + len(subsequence)]\n",
    "\n",
    "        # Use difflib's SequenceMatcher to calculate similarity.\n",
    "        similarity = SequenceMatcher(None, subsequence, window).ratio()\n",
    "\n",
    "        # If the similarity exceeds the cutoff, consider it a match.\n",
    "        if similarity >= cutoff:\n",
    "            start_index = i\n",
    "            break\n",
    "\n",
    "    return start_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "9727d40d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def exportNewScriptRow(matched_row,text):\n",
    "    start_time_str = matched_row[\"Start Time\"]\n",
    "    start_time = convert_time_to_seconds(start_time_str)\n",
    "    transcript_duration=convert_time_to_seconds(matched_row[\"End Time\"]) - convert_time_to_seconds(matched_row[\"Start Time\"])                    \n",
    "    offset = find_start_character_index(matched_row[\"EN\"],text[:8], cutoff=.8)\n",
    "    seconds_per_letter =transcript_duration / len(matched_row[\"EN\"])\n",
    "    start_time+=offset*seconds_per_letter\n",
    "    duration_seconds = len(text) * seconds_per_letter\n",
    "    end_time = start_time + duration_seconds\n",
    "\n",
    "    start_time_mmss = format_time(int(start_time))\n",
    "    end_time_mmss = format_time(int(end_time))\n",
    "    return start_time_mmss,end_time_mmss,duration_seconds\n",
    "\n",
    "# Function to match script text with transcript\n",
    "def match_script_with_transcript(docx_file_path, transcript_df):\n",
    "    doc = docx.Document(docx_file_path)\n",
    "\n",
    "    speaker = \"\"\n",
    "    next_row={\"EN\":\"temp\"}\n",
    "    matched_row={\"EN\":\"temp\"}\n",
    "    transcript_lines = []\n",
    "    for line in doc.paragraphs:\n",
    "        text = str(line.text.strip())\n",
    "        if any(char.isalpha() for char in text):\n",
    "            match = re.match(r'\\*([^, ]+)', text)\n",
    "            if match:##if the line is defining a new speaker because it in eht form *Magalys\n",
    "                speaker = match.group(1)\n",
    "            else:\n",
    "\n",
    "                speaker_df=transcript_df[transcript_df[\"Speaker\"].str.contains(speaker.lower())]  \n",
    "\n",
    "                ##first try the next line after the previously searched for line\n",
    "                if text in next_row[\"EN\"]:\n",
    "                    matched_row=next_row\n",
    "                    \n",
    "                elif text in matched_row[\"EN\"]:\n",
    "                    matched_row=matched_row\n",
    "                    \n",
    "                else:\n",
    "                    closest_match = get_close_matches(text, speaker_df[\"EN\"].apply(float_to_str), n=1, cutoff=0.6)\n",
    "                    if closest_match:\n",
    "                        print(closest_match,text)\n",
    "\n",
    "                        matched_row = speaker_df[speaker_df[\"EN\"] == closest_match[0]].iloc[0]\n",
    "\n",
    "\n",
    "                if closest_match or next_row[\"EN\"]==text:\n",
    "\n",
    "                    #this is a hacky way to get the next row but i could figure out a good index method\n",
    "                    getNextRow=False\n",
    "                    for index,row in speaker_df.iterrows():\n",
    "                        if getNextRow:\n",
    "                            next_row=row\n",
    "                            getNextRow=False\n",
    "                        elif row[\"EN\"]==matched_row[\"EN\"]:\n",
    "                            getNextRow=True\n",
    "\n",
    "                    start_time_mmss,end_time_mmss,duration_seconds=exportNewScriptRow(matched_row,text)\n",
    "                    interview=matched_row[\"Interview\"]\n",
    "                else:\n",
    "                    start_time_mmss=\"\"\n",
    "                    end_time_mmss=\"\"\n",
    "                    duration_seconds=\"\"\n",
    "                    interview=\"can't find\"\n",
    "\n",
    "                transcript_lines.append([speaker,interview, start_time_mmss, end_time_mmss, duration_seconds, text])                \n",
    "\n",
    "    return transcript_lines\n",
    "\n",
    "# Function to write matched transcript to CSV (unchanged)\n",
    "def write_matched_transcript_to_csv(transcript_lines, output_csv_path):\n",
    "    with open(output_csv_path, \"w\", newline=\"\") as csvfile:\n",
    "        fieldnames = [\"Speaker\", \"Interview\",\"Start Time\", \"End Time\", \"Duration (Seconds)\", \"Transcript\"]\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames,quoting)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for line in transcript_lines:\n",
    "            speaker, interview, start_time_mmss, end_time_mmss, duration_seconds, text = line\n",
    "            writer.writerow({\"Speaker\": speaker,\"Interview\":interview, \"Start Time\": start_time_mmss, \"End Time\": end_time_mmss, \"Duration (Seconds)\": duration_seconds, \"Transcript\": text})\n",
    "\n",
    "def float_to_str(value):\n",
    "    return str(value)\n",
    "            \n",
    "# #test\n",
    "# script_path = \"../01_Scripts/Wayuu/Dunas Script.docx\"\n",
    "# transcript_csv = pd.read_csv(\"../04_Interview CSV/Wayuu/dunas.csv\")\n",
    "# transcript_csv[\"Interview\"]=\"dunas\"\n",
    "# output_csv_path = \"../01_Scripts/Wayuu/dunas-script.csv\"\n",
    "\n",
    "# transcript_lines = match_script_with_transcript(script_path, transcript_csv)\n",
    "# print(transcript_lines)\n",
    "# write_matched_transcript_to_csv(transcript_lines, output_csv_path)\n",
    "# print(f\"CSV file with speaker, start time, end time, duration, and matched transcript created at {output_csv_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d12b70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "6d19231a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['These are,'] These are,\n",
      "02:20\n",
      "02:23\n",
      "02:23\n",
      "02:25\n",
      "02:25\n",
      "02:27\n",
      "02:27\n",
      "02:29\n",
      "02:29\n",
      "02:32\n",
      "02:32\n",
      "02:35\n",
      "02:35\n",
      "02:38\n",
      "02:38\n",
      "02:41\n",
      "02:42\n",
      "02:44\n",
      "02:44\n",
      "02:47\n",
      "02:47\n",
      "02:49\n",
      "['This is the control that regulates the load'] This is the control that regulates the load\n",
      "07:53\n",
      "07:58\n",
      "07:59\n",
      "08:01\n",
      "08:01\n",
      "08:02\n",
      "08:02\n",
      "08:06\n",
      "08:06\n",
      "08:07\n",
      "08:07\n",
      "08:10\n",
      "08:10\n",
      "08:13\n",
      "08:13\n",
      "08:17\n",
      "08:17\n",
      "08:18\n",
      "08:18\n",
      "08:21\n",
      "['Well Neko, you are entering the Macuira mountain range.\\nThese are land of the Arpushana (family lineage)'] Well Neko, you are entering the Macuira mountain range.\n",
      "00:01\n",
      "00:04\n",
      "00:04\n",
      "00:07\n",
      "00:10\n",
      "00:14\n",
      "00:16\n",
      "00:18\n",
      "00:18\n",
      "00:19\n",
      "00:24\n",
      "00:27\n",
      "00:27\n",
      "00:29\n",
      "00:31\n",
      "00:36\n",
      "00:37\n",
      "00:41\n",
      "00:41\n",
      "00:42\n",
      "[\"That's why I remain engraved in the stone of destiny\\nas they say, there, at sword point\"] That's why I remain engraved in the stone of destiny\n",
      "06:27\n",
      "06:31\n",
      "06:31\n",
      "06:33\n",
      "['Where now'] Where now***\n",
      "06:35\n",
      "06:36\n",
      "['They all go'] They all go***\n",
      "06:38\n",
      "06:39\n",
      "06:40\n",
      "06:43\n",
      "06:45\n",
      "06:46\n",
      "06:49\n",
      "06:51\n",
      "06:52\n",
      "06:54\n",
      "06:55\n",
      "06:58\n",
      "07:00\n",
      "07:02\n",
      "['During and after this ritual you are given some\\nWayuu medicinal plants'] During and after this ritual you are given some\n",
      "04:02\n",
      "04:06\n",
      "04:06\n",
      "04:07\n",
      "04:09\n",
      "04:13\n",
      "04:14\n",
      "04:17\n",
      "04:17\n",
      "04:17\n",
      "04:21\n",
      "04:26\n",
      "04:26\n",
      "04:27\n",
      "04:31\n",
      "04:35\n",
      "04:35\n",
      "04:36\n",
      "04:39\n",
      "04:42\n",
      "04:42\n",
      "04:43\n",
      "04:48\n",
      "04:50\n",
      "04:52\n",
      "04:54\n",
      "04:55\n",
      "04:59\n",
      "['The old say that there is a spirituality of everything'] The old say that there is a spirituality of everything\n",
      "27:18\n",
      "27:24\n",
      "27:25\n",
      "27:28\n",
      "27:28\n",
      "27:31\n",
      "27:35\n",
      "27:38\n",
      "27:38\n",
      "27:40\n",
      "27:44\n",
      "27:50\n",
      "27:50\n",
      "27:51\n",
      "27:53\n",
      "27:56\n",
      "27:56\n",
      "27:57\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# #test\n",
    "script_path = \"../01_Scripts/Wayuu/Magalys test.docx\"\n",
    "concat_transcripts = concatenate_transcript_csvs(\"../04_Interview CSV/Wayuu/\")\n",
    "output_csv_path = script_path[:-5]+\".csv\"\n",
    "transcript_lines = match_script_with_transcript(script_path, concat_transcripts)\n",
    "write_matched_transcript_to_csv(transcript_lines, output_csv_path)\n",
    "\n",
    "print(\"Done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12264372",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca7d59d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6284b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f27181",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
