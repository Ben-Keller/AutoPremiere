{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "7e27f65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import docx\n",
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz\n",
    "from collections import defaultdict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "7209e307",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id=\"Varanasi\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "ccf10633",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts_dir = \"../02_Transcripts/\"+project_id\n",
    "output_csv_dir = \"../04_Interview CSV/\"+project_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45412874",
   "metadata": {},
   "source": [
    "<h3> Transcripts to CSVs </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "9dfbba83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sadhu 1': {'EN': '../02_Transcripts/Varanasi/Transcripts (EN)/Sadhu 1 (EN).docx', 'HI': '../02_Transcripts/Varanasi/Transcripts (HI)/Sadhu 1 (HI).docx'}, 'sadhu 2': {'EN': '../02_Transcripts/Varanasi/Transcripts (EN)/Sadhu 2 (EN).docx', 'HI': '../02_Transcripts/Varanasi/Transcripts (HI)/Sadhu 2 (HI).docx'}, 'kashi baba': {'EN': '../02_Transcripts/Varanasi/Transcripts (EN)/Kashi Baba (EN).docx', 'HI': '../02_Transcripts/Varanasi/Transcripts (HI)/Kashi Baba (HI).docx'}}\n"
     ]
    }
   ],
   "source": [
    "def get_languages_and_interviews(transcripts_dir):\n",
    "    interviews_dict = {}  # Dictionary to store interviews and their languages and file paths\n",
    "\n",
    "    for language_folder in os.listdir(transcripts_dir):\n",
    "        language_dir_path = os.path.join(transcripts_dir, language_folder)\n",
    "        \n",
    "        # Check if it's a valid language folder (e.g., \"Transcripts (EN)\")\n",
    "        if os.path.isdir(language_dir_path) and language_folder.startswith(\"Transcripts (\"):\n",
    "            language = language_folder.split(\" (\")[1].split(\")\")[0]\n",
    "            \n",
    "            for interview_file in os.listdir(language_dir_path):\n",
    "                if interview_file.endswith(\".docx\") and not os.path.basename(interview_file).startswith(\"~$\"):\n",
    "                    interview_name, ext = os.path.splitext(interview_file)\n",
    "                    interview_code = interview_name.split(\"(\")[0].strip().lower()  # Extract the interview code (e.g., \"dunas\")\n",
    "                    interview_path = os.path.join(language_dir_path, interview_file)\n",
    "                    \n",
    "                    # Create or update the entry for this interview code\n",
    "                    if interview_code not in interviews_dict:\n",
    "                        interviews_dict[interview_code] = {}\n",
    "                    \n",
    "                    # Append language and file path to the input_files dictionary\n",
    "                    interviews_dict[interview_code][language] = interview_path\n",
    "\n",
    "    return interviews_dict \n",
    "\n",
    "interviews_dict=get_languages_and_interviews(transcripts_dir)\n",
    "print(interviews_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "c4d795f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_subtitle(text):\n",
    "    # Remove asterisks and text between brackets\n",
    "    text = re.sub(r'\\*|\\[.*?\\]', '', text)\n",
    "    text = re.sub(r'[\\u2012\\u2013\\u2014\\u2015]', '-', text)\n",
    "    text = text.replace(\";\",\":\")\n",
    "    return text.strip()\n",
    "\n",
    "def convert_language_to_csv(language, input_file, subtitles):\n",
    "    doc = docx.Document(input_file)\n",
    "    subtitle_block = \"\"\n",
    "    current_time_range = None\n",
    "    current_highlight_color = None\n",
    "    speaker=None\n",
    "        \n",
    "    for paragraph in doc.paragraphs:\n",
    "        text = paragraph.text.strip()\n",
    "        text = clean_subtitle(text)  # Clean the subtitle text\n",
    "        # Check if the paragraph contains a time range\n",
    "        time_range_match = re.match(r'^(\\d+[:;]\\d+(?::\\d+)?(?:[.,]\\d+)?\\s*-\\s*\\d+[:;]\\d+(?::\\d+)?(?:[.,]\\d+)?)\\s*\\**', text)\n",
    "            #r'^(\\d+[:;]\\d+(?::\\d+){0,2}\\s*-\\s*\\d+[:;]\\d+(?::\\d+){0,2})\\s*\\**'                    \n",
    "        if len(text)>0 and not text.isnumeric():\n",
    "            if time_range_match:\n",
    "                time_range = time_range_match.group(1)\n",
    "                # If there's a previous subtitle block, add it to the dictionary\n",
    "                if subtitle_block and current_time_range:\n",
    "                    subtitles[current_time_range][language] = {\n",
    "                        \"text\": subtitle_block,\n",
    "                        \"highlight_color\": current_highlight_color,\n",
    "                        \"comments\": \"\",  # Initialize an empty \"Comments\" column\n",
    "                        \"speaker\":speaker\n",
    "                    }\n",
    "                subtitle_block = \"\"\n",
    "                current_time_range = time_range\n",
    "                current_highlight_color = None  # Reset the highlight color for a new time range\n",
    "            elif text[0]==\"#\":\n",
    "                speaker = re.sub(r'\\[.#?\\]', '', text.lower()[1:])\n",
    "\n",
    "            else:\n",
    "                #add this subtitle block\n",
    "                subtitle_block += text + \"\\n\"\n",
    "                # Extract the highlight color for each run in the paragraph\n",
    "                for run in paragraph.runs:\n",
    "                    if run.font.highlight_color is not None:\n",
    "                        current_highlight_color = run.font.highlight_color\n",
    "\n",
    "\n",
    "    # Add the last subtitle block to the dictionary\n",
    "    if subtitle_block and current_time_range:\n",
    "        subtitles[current_time_range][language] = {\n",
    "            \"text\": subtitle_block,\n",
    "            \"highlight_color\": current_highlight_color,\n",
    "            \"comments\": \"\",  # Initialize an empty \"Comments\" column\n",
    "            \"speaker\":speaker\n",
    "        }\n",
    "\n",
    "def interview_to_csv(input_files, output_csv_file):\n",
    "    # Initialize a dictionary to store subtitles for all languages\n",
    "    all_subtitles = defaultdict(lambda: defaultdict(str))\n",
    "\n",
    "    # Iterate through each language and convert to CSV\n",
    "    for language, input_file in input_files.items():\n",
    "        convert_language_to_csv(language, input_file, all_subtitles)\n",
    "\n",
    "    # Write the combined subtitles to the CSV file\n",
    "    with open(output_csv_file, 'w', newline='', encoding='utf-8') as f:\n",
    "        csv_writer = csv.writer(f)\n",
    "        # Write the header row\n",
    "        header = [\"Start Time\", \"End Time\"] + list(input_files.keys()) + [\"Highlight\", \"Comments\",\"Speaker\"]\n",
    "        csv_writer.writerow(header)\n",
    "        # Write the data rows with all languages stacked\n",
    "        for time_range, language_subtitles in all_subtitles.items():\n",
    "            try:\n",
    "                start_time, end_time = map(str.strip, time_range.split(\"-\"))\n",
    "                highlight_color = language_subtitles[list(input_files.keys())[0]][\"highlight_color\"]\n",
    "                speaker=language_subtitles[list(input_files.keys())[0]][\"speaker\"]\n",
    "                row = [start_time, end_time] + [clean_subtitle(language_subtitles.get(lang, {\"text\": \"\"})[\"text\"]) for lang in input_files.keys()] + [highlight_color, \"\",speaker]\n",
    "                csv_writer.writerow(row)\n",
    "            except:\n",
    "                print(\"broken: \", time_range)#,language_subtitles)\n",
    "\n",
    "    print(f\"Conversion completed. Data saved to {output_csv_file}\")\n",
    "    \n",
    "# # # Test\n",
    "# test_interview_id = next(iter(interviews_dict))\n",
    "# input_files=interviews_dict[test_interview_id]\n",
    "# output_csv_file = \"../04_Interview CSV/\"+project_id+\"/\"+test_interview_id+\".csv\"\n",
    "# # # Convert all languages to CSV using the pipeline function\n",
    "# interview_to_csv(input_files, output_csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f2478a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "d48d0dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sadhu 1\n",
      "Conversion completed. Data saved to ../04_Interview CSV/Varanasi/sadhu 1.csv\n",
      "sadhu 2\n",
      "Conversion completed. Data saved to ../04_Interview CSV/Varanasi/sadhu 2.csv\n",
      "kashi baba\n",
      "Conversion completed. Data saved to ../04_Interview CSV/Varanasi/kashi baba.csv\n"
     ]
    }
   ],
   "source": [
    "# Process all interviews in the transcripts directory\n",
    "\n",
    "def process_all_interviews(interviews_dict, output_csv_dir):\n",
    "    # Get the combined dictionary of languages and interviews\n",
    "    interviews_dict = get_languages_and_interviews(transcripts_dir)\n",
    "    \n",
    "    for interview_code in interviews_dict.keys():\n",
    "        print(interview_code)\n",
    "        # Process the interview using the input_files and interview_code\n",
    "        output_csv_file = os.path.join(output_csv_dir, f\"{interview_code}.csv\")\n",
    "        interview_to_csv(interviews_dict[interview_code], output_csv_file)\n",
    "        \n",
    "process_all_interviews(interviews_dict, output_csv_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b429ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "a54cb2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "##these are used when converting scripts to csv files\n",
    "def concatenate_transcript_csvs(folder_path):\n",
    "\n",
    "    # Get a list of all CSV files in the folder\n",
    "    csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv') and \"-merged\" not in f]\n",
    "\n",
    "    # Initialize an empty DataFrame\n",
    "    concatenated_df = pd.DataFrame()\n",
    "\n",
    "    # Iterate through CSV files and concatenate them\n",
    "    for csv_file in csv_files:\n",
    "        interview_code = os.path.splitext(csv_file)[0]  # Extract interview code from file name\n",
    "        csv_path = os.path.join(folder_path, csv_file)\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        # Add an \"Interview\" column with the interview code\n",
    "        df['Interview'] = interview_code\n",
    "\n",
    "        # Concatenate the dataframes\n",
    "        concatenated_df = pd.concat([concatenated_df, df], ignore_index=True)\n",
    "        \n",
    "    concatenated_df.to_csv(\"../04_Interview CSV/\"+project_id+\"/\"+project_id+\"-merged.csv\",index=False)\n",
    "\n",
    "    return concatenated_df\n",
    "\n",
    "folder_path = \"../04_Interview CSV/\"+project_id\n",
    "concatenated_transcripts = concatenate_transcript_csvs(folder_path)\n",
    "#print(concatenated_transcripts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64031e0b",
   "metadata": {},
   "source": [
    "<h3>CSVs to SRTs</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "e642885f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45192\n",
      "155\n",
      "1595\n",
      "6.486\n"
     ]
    }
   ],
   "source": [
    "def timecode_to_frames(text):\n",
    "    if len(text.split(\":\"))==2:\n",
    "        frames=(int(text.split(\":\")[0])*60+int(text.split(\":\")[1]))*24\n",
    "    elif len(text.split(\":\"))==3:\n",
    "        frames=int((int(text.split(\":\")[0])*3600+int(text.split(\":\")[1])*60+float(text.split(\":\")[2].replace(\",\",\".\")))*24)\n",
    "    elif len(text.split(\":\"))==4:\n",
    "        frames=(int(text.split(\":\")[0])*3600+int(text.split(\":\")[1])*60+int(text.split(\":\")[2]))*24+int(text.split(\":\")[3])\n",
    "    else:\n",
    "        print(text+\"timecode parse error\")\n",
    "    return frames\n",
    "\n",
    "print(timecode_to_frames(\"00:31:23\"))  #test\n",
    "print(timecode_to_frames(\"00:00:06,486\"))  #test\n",
    "print(timecode_to_frames(\"00:01:06,486\"))  #test\n",
    "print(float(\"06.486\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "8dbb9bc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'00:31:23,083'"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def frames_to_srt_timecode(frames,fast=False):\n",
    "    if fast==True:\n",
    "        frames=frames*24/23.976 ##this is dumb, but solves the error when importing into premiere\n",
    "    frames=int(frames)\n",
    "    hours = frames // (3600*24)\n",
    "    remaining_frames = frames % (3600*24)\n",
    "    minutes=remaining_frames // (60*24)\n",
    "    remaining_frames=remaining_frames % (60*24)\n",
    "    seconds=remaining_frames // (24)\n",
    "    remaining_frames =remaining_frames %(24)\n",
    "    frames=remaining_frames\n",
    "\n",
    "    timecode=\"{:02d}\".format(hours)+\":\"+\"{:02d}\".format(minutes)+\":\"+\"{:02d}\".format(seconds)+\",\"+\"{:03d}\".format(int(frames/24*1000))\n",
    "\n",
    "    return timecode\n",
    "\n",
    "frames_to_srt_timecode(45194)   #test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "a71ce7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sadhu 1\n",
      "['HI', 'EN']\n",
      "sadhu 2\n",
      "['HI', 'EN']\n",
      "kashi baba\n",
      "['HI', 'EN']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Input and output directories\n",
    "csv_dir = \"../04_Interview CSV/\"+project_id\n",
    "subtitles_dir = \"../05_Subtitles/\"+project_id\n",
    "\n",
    "##this fastSubs stuff needs to be refactored, it is just a temp fix to errors in framerate exports 24 and 23.95\n",
    "#fastSubs=[\"romelia\",\"joaquin\",\"magalys electrico\",\"neko weaving\",\"weildler inside\",\"weildler outside\",\"bailarinas\"]\n",
    "fastSubs=[\"coca\",\"fuego\",\"arregoces\",\"tejiendo\"]\n",
    "\n",
    "# Function to convert a CSV file to SRT for multiple languages\n",
    "def csv_to_srt_multiple_languages(csv_file):\n",
    "\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    #this should be removed, only is needed because of an error in exporting videos at 23.976 vs 24fps\n",
    "    code=csv_file.split(\"/\")[-1].split(\".csv\")[0].lower()\n",
    "    print(code)\n",
    "    fast=False\n",
    "    if code in fastSubs:\n",
    "        fast=True\n",
    "\n",
    "    # Get the language columns dynamically (excluding specific columns)\n",
    "    exclude_columns = [\"Start Time\", \"End Time\", \"Highlight\", \"Comments\",\"Speaker\"]\n",
    "    language_columns = [col for col in df.columns if col not in exclude_columns]\n",
    "    language_columns.reverse()#this is to make english last,\n",
    "    \n",
    "    print(language_columns)\n",
    "\n",
    "    # Initialize a dictionary to store lines for each language\n",
    "    language_srt_lines = {col: [] for col in language_columns}\n",
    "    combined_srt_lines = []\n",
    "    counter = 1\n",
    "\n",
    "    # Loop through each row in the CSV\n",
    "    for index, row in df.iterrows():\n",
    "        # Format the timestamps in SRT format\n",
    "        srt_timecode = f\"{counter}\\n{frames_to_srt_timecode(timecode_to_frames(row['Start Time']),fast)} --> {frames_to_srt_timecode(timecode_to_frames(row['End Time']),fast)}\"\n",
    "\n",
    "        # Append the text for each language to their respective lines\n",
    "        for lang in language_columns:\n",
    "            language_srt_lines[lang].append(srt_timecode)\n",
    "            language_srt_lines[lang].append(f\"{row[lang]}\\n\")\n",
    "\n",
    "        # Append the text to the combined SRT\n",
    "        combined_srt_lines.append(srt_timecode)\n",
    "        combined_srt_lines.append('\\n----\\n'.join([str(row[lang]) for lang in language_columns]) + '\\n')\n",
    "\n",
    "        # Increment the counter\n",
    "        counter += 1\n",
    "\n",
    "    # Determine the output SRT file paths for combined and individual SRTs\n",
    "    base_filename = os.path.splitext(os.path.basename(csv_file))[0]\n",
    "    combined_langs = '-'.join(language_columns)\n",
    "    combined_srt_filename = f\"{base_filename} ({combined_langs}).srt\"\n",
    "    combined_srt_dir = os.path.join(subtitles_dir,\"SRT Export (\"+combined_langs+\")\")\n",
    "    os.makedirs(combined_srt_dir, exist_ok=True)\n",
    "\n",
    "    combined_srt_path= os.path.join(combined_srt_dir, combined_srt_filename)\n",
    "\n",
    "    # Create the folder for combined SRT\n",
    "    os.makedirs(subtitles_dir, exist_ok=True)\n",
    "\n",
    "    # Write the combined SRT file\n",
    "    with open(combined_srt_path, 'w', encoding='utf-8') as combined_srt_file:\n",
    "        combined_srt_file.write('\\n'.join(combined_srt_lines))\n",
    "\n",
    "    # Create folders for each language and write the individual SRT files\n",
    "    for lang in language_columns:\n",
    "        lang_output_dir = os.path.join(subtitles_dir, f\"SRT Export ({lang})\")\n",
    "        os.makedirs(lang_output_dir, exist_ok=True)\n",
    "        lang_srt_filename = f\"{base_filename} ({lang}).srt\"\n",
    "        lang_srt_path = os.path.join(lang_output_dir, lang_srt_filename)\n",
    "        with open(lang_srt_path, 'w', encoding='utf-8') as lang_srt_file:\n",
    "            lang_srt_file.write('\\n'.join(language_srt_lines[lang]))\n",
    "\n",
    "    #print(f'Combined SRT file \"{combined_srt_path}\" and individual SRT files have been created.')\n",
    "\n",
    "\n",
    "# Process all CSV files in the input directory\n",
    "for filename in os.listdir(csv_dir):\n",
    "    if filename.endswith('.csv') and \"-merged\" not in filename:\n",
    "        csv_file = os.path.join(csv_dir, filename)\n",
    "        csv_to_srt_multiple_languages(csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8a4c6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8518c1e1",
   "metadata": {},
   "source": [
    "<h3> Check status of txts, csv, xmls, srts </h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "c885287e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getCodesPresent(folderpath,ext):\n",
    "    codes = []\n",
    "    for filename in os.listdir(folderpath):\n",
    "        if filename.endswith(ext) and os.path.getsize(os.path.join(folderpath,filename))>1000:\n",
    "            code=filename.lower().split(ext.lower())[0].strip()\n",
    "            codes.append(code)\n",
    "    return codes\n",
    "\n",
    "enTexts=getCodesPresent('../02_Transcripts/'+project_id+'/Transcripts (EN)/',' (EN).docx')\n",
    "# esTexts=getCodesPresent('../02_Transcripts/'+project_id+'/Transcripts (ES)/',' (ES).docx')\n",
    "xmls=getCodesPresent('../03_Interview XML/'+project_id+'/','- final.xml')\n",
    "enSubs=getCodesPresent('../05_Subtitles/'+project_id+'/SRT Export (EN)/',' (EN).srt')\n",
    "# esSubs=getCodesPresent('../05_Subtitles/'+project_id+'/SRT Export (ES)/',' (ES).srt')\n",
    "\n",
    "lists_dict ={\"EN Transcripts\":enTexts,\n",
    "#             \"ES Transcripts\":esTexts,\n",
    "            \"EN Subs\":enSubs,\n",
    "#             \"ES Subs\":esSubs,\n",
    "            \"XMLs\":xmls}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "f29a7a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            EN Transcripts  EN Subs   XMLs  Undefined Count\n",
      "code                                                       \n",
      "kashi baba            True     True  False                0\n",
      "sadhu 2               True     True  False                0\n",
      "sadhu 1               True     True  False                0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a DataFrame with unique codes\n",
    "unique_codes = list(set(code for sublist in lists_dict.values() for code in sublist))\n",
    "df = pd.DataFrame({'code': unique_codes})\n",
    "\n",
    "# Add columns for each list with True/False values\n",
    "for list_name, code_list in lists_dict.items():\n",
    "    df[list_name] = df['code'].isin(code_list)\n",
    "\n",
    "def getUndefinedCount(code):\n",
    "    try:\n",
    "        docx_file='../02_Transcripts/'+project_id+'/Transcripts (EN)/'+code+' (EN).docx'  \n",
    "        search_string=\"xxx\"\n",
    "        count = 0\n",
    "        # Load the DOCX document\n",
    "        doc = docx.Document(docx_file)\n",
    "        # Iterate through paragraphs and search for the string (case-insensitive)\n",
    "        for paragraph in doc.paragraphs:\n",
    "            if search_string.lower() in paragraph.text.lower():\n",
    "                count += paragraph.text.lower().count(search_string.lower())\n",
    "        return count\n",
    "    except:\n",
    "        return \"\"\n",
    "    \n",
    "# Add column for count of XXX in document    \n",
    "df[\"Undefined Count\"]=df[\"code\"].apply(getUndefinedCount)\n",
    "    \n",
    "# Set 'code' as the index\n",
    "df.set_index('code', inplace=True)\n",
    " \n",
    "# Print the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50426b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
